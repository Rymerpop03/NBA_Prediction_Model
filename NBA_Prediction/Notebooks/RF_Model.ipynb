{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2349464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de3f95",
   "metadata": {},
   "source": [
    "# NBA Game Prediction Model\n",
    "\n",
    "This notebook implements a Random Forest model to predict NBA game outcomes based on team statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067ddcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read interim data\n",
    "train_data = pd.read_csv('C:/Users/poke5/Desktop/Projects/NBA_Prediction/data/processed/train.csv')\n",
    "test_data = pd.read_csv('C:/Users/poke5/Desktop/Projects/NBA_Prediction/data/processed/test.csv')\n",
    "validation_data = pd.read_csv('C:/Users/poke5/Desktop/Projects/NBA_Prediction/data/processed/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae0e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matchup_df(df):\n",
    "    \"\"\"Create simple feature matrix from games data\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for gid, game in df.groupby('GAME_ID'):\n",
    "        if len(game) != 2:\n",
    "            continue\n",
    "            \n",
    "        team1 = game.iloc[0]\n",
    "        team2 = game.iloc[1]\n",
    "        \n",
    "        # Get just the rolling features\n",
    "        feature_cols = [col for col in df.columns if col.startswith('r10_')]\n",
    "        \n",
    "        # Calculate differences\n",
    "        feature_vector = []\n",
    "        for col in feature_cols:\n",
    "            diff = float(team1[col]) - float(team2[col])\n",
    "            feature_vector.append(diff)\n",
    "            \n",
    "        # Add home advantage\n",
    "        feature_vector.append(1 if 'vs.' in team1['MATCHUP'] else 0)\n",
    "        \n",
    "        # Add to lists\n",
    "        features.append(feature_vector)\n",
    "        labels.append(1 if team1['WL'] == 'W' else 0)\n",
    "            \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf5c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (11650, 8)\n",
      "Number of games: 11650\n",
      "Number of features: 8\n",
      "Class balance (wins/losses): {np.int64(0): np.int64(5821), np.int64(1): np.int64(5829)}\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "X, y = make_matchup_df(train_data)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of games: {len(X)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Class balance (wins/losses):\", dict(zip(*np.unique(y, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548de400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "Training set size: 9320 samples\n",
      "Validation set size: 2330 samples\n",
      "Accuracy on validation set: 0.604\n"
     ]
    }
   ],
   "source": [
    "# Train/test split and model training\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_tr, y_tr)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('\\nModel Evaluation:')\n",
    "print(f'Training set size: {len(X_tr)} samples')\n",
    "print(f'Validation set size: {len(X_val)} samples')\n",
    "print(f'Accuracy on validation set: {accuracy_score(y_val, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b2907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_matchup(team1_abbr, team2_abbr, is_team1_home=True):\n",
    "    \"\"\"Simple prediction for a matchup between two teams\"\"\"\n",
    "    # Get most recent stats for both teams\n",
    "    team1_data = train_data[train_data['TEAM_ABBREVIATION'] == team1_abbr].iloc[-1]\n",
    "    team2_data = train_data[train_data['TEAM_ABBREVIATION'] == team2_abbr].iloc[-1]\n",
    "    \n",
    "    # Just use the basic rolling stats and home advantage\n",
    "    features_to_use = [col for col in train_data.columns if col.startswith('r10_')]\n",
    "    \n",
    "    # Build feature vector (differences between teams)\n",
    "    feature_vector = []\n",
    "    for col in features_to_use:\n",
    "        diff = float(team1_data[col]) - float(team2_data[col])\n",
    "        feature_vector.append(diff)\n",
    "    \n",
    "    # Add home court advantage\n",
    "    feature_vector.append(1 if is_team1_home else 0)\n",
    "    \n",
    "    # Make prediction\n",
    "    X_pred = np.array([feature_vector])\n",
    "    prob = clf.predict_proba(X_pred)[0][1]\n",
    "    pred = clf.predict(X_pred)[0]\n",
    "    winner = team1_abbr if pred == 1 else team2_abbr\n",
    "    \n",
    "    return {\n",
    "        'winner': winner,\n",
    "        'probability': prob\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ad2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    r10_MissedFG r10_MissedFT  r10_TSA r10_TS_Pct r10_FG_Eff r10_RebRatio  \\\n",
      "POR         48.3          4.8  101.672  58.587483   0.472266     0.224538   \n",
      "HOU         50.1          4.4  101.376  58.583486   0.440646     0.191088   \n",
      "BOS         46.3          5.2   98.884  58.723955   0.465237     0.210166   \n",
      "MIA         48.1          4.8   97.572  56.989861   0.444789     0.191539   \n",
      "UTA         44.8          5.4   94.756  58.827994   0.467843     0.186104   \n",
      "\n",
      "       r10_TS  \n",
      "POR  0.585875  \n",
      "HOU  0.585835  \n",
      "BOS   0.58724  \n",
      "MIA  0.569899  \n",
      "UTA   0.58828  \n"
     ]
    }
   ],
   "source": [
    "# Calculate team averages using the latest stats for each team\n",
    "features_to_use = [col for col in train_data.columns if col.startswith('r10_')]\n",
    "team_avgs = {}\n",
    "\n",
    "for team in train_data['TEAM_ABBREVIATION'].unique():\n",
    "    team_recent = train_data[train_data['TEAM_ABBREVIATION'] == team].iloc[-1]\n",
    "    team_avgs[team] = team_recent[features_to_use]\n",
    "\n",
    "team_avgs = pd.DataFrame(team_avgs).T \n",
    "print(team_avgs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47276df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and team averages exported for use in PredictionSimulator\n"
     ]
    }
   ],
   "source": [
    "# Save model and team averages for PredictionSimulator\n",
    "joblib.dump(clf, 'rf_model.pkl')\n",
    "team_avgs.to_csv('team_averages.csv')\n",
    "print(\"Model and team averages exported for use in PredictionSimulator\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
